\subtitle{Surrogate Models}
\begin{document}
\maketitle
%-----------------------------------------------------------------------
\myframetop{Desiderata for Surrogate Models in Bayesian Optimization}{

    \begin{columns}[T] % align columns
    \begin{column}{.48\textwidth}
    \only<1-2>{
        \begin{block}{In all cases}
        \begin{itemize}
        	\item Regression model with uncertainty estimates
        	\item Accurate predictions
        \end{itemize}
        \end{block}
    }
    \only<2-2>{
        \begin{block}{Depending on the application}
        \begin{itemize}
        	\item Is cheap to train
        	\item Scales well in the number of data points
        	\item Scales well in the number of dimensions
        	\item Can handle different types of inputs (categorical and continuous)
        \end{itemize}
        \end{block}
    }
    \end{column}%
    
    \hfill%
    
    \begin{column}{.48\textwidth}
    \bigskip
    \bigskip
    %\only<1-1>{\includegraphics[width=1.\textwidth]{images/bo_loop_overview/03_mean.png}}
    %\only<2-6>{
    \includegraphics[width=\textwidth]{images/bo_loop_overview/Uncertainty.pdf}
    %}
    
    \end{column}%
    \end{columns}
}
%-----------------------------------------------------------------------

%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}[c]{Overview of the Surrogate Models We'll Discuss}

%\begin{columns}[T] % align columns
%\begin{column}{.38\textwidth}
%\begin{minipage}[c][.6\textheight][c]{\linewidth}
\begin{itemize}
	\item Gaussian Processes \note[item]{(quite common)}
	\item Random Forests \note[item]{(our default choice)}
	\item Bayesian Neural Networks \note[item]{(recent trend)}

\end{itemize}
%\end{minipage}
%\end{column}%

%\hfill%
%\begin{column}{.58\textwidth}
%
%\begin{columns}[T] % align columns
%\begin{column}{.48\textwidth}
%    \includegraphics[width=1.\textwidth]{images/surrogate_models/uncertainty_gp.jpg}
%\end{column}%
%
%\hfill%
%
%\begin{column}{.48\textwidth}
%    \includegraphics[width=1.\textwidth]{images/surrogate_models/uncertainty_forest.jpg}
%\end{column}%
%\end{columns}
%
%\vspace*{\fill}
%\begin{center}
%  \includegraphics[width=.6\textwidth]{images/surrogate_models/uncertainty_dngo.jpg}
%  
%\end{center}
%\vspace*{\fill}
%
%\end{column}%
%\end{columns}
%
%\hspace{5.5cm}\footnotesize{Image source: \lit{\href{}{A. Klein: Introduction Automated Machine Learning}}}


\end{frame}
%-----------------------------------------------------------------------
\begin{frame}[c]{Gaussian Processes (GPs): Reminder of Pros and Cons}

\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}

    \begin{block}{Advantages}
    \begin{itemize}
    	\item Smooth and reliable uncertainty estimates 
		\item Strong sample efficiency
    	\item We can encode expert knowledge about the design space in the kernel 
    \end{itemize}
    \end{block}
\bigskip
\pause
\hspace*{0.5cm}These advantages make GPs the\\
\hspace*{0.5cm}\alert{most commonly-used model\\
\hspace*{0.5cm}in Bayesian optimization}

\end{column}%

\hfill%
\pause

\begin{column}{.48\textwidth}
    \begin{block}{Disadvantages}
    \begin{itemize}
    	\item Performance can be quite sensitive to the choice of kernel
    	\note[item]{(if we don't optimize small-dimensional, continuous functions)}
    	\item Cost scales cubically with the number of observations 
    	\item Weak performance for high dimensionality
    	\note[item]{(because of inverting the kernel)}
    	\item Not easily applicable in discrete or conditional spaces 
    	\item Sensitive to its own hyperparameters
    \end{itemize}
\end{block}

\end{column}
\end{columns}

\note[item]{
	\begin{itemize}
		 \item e.g., special kernels for categorical hyperparameters and\\ conditional dependencies
	\end{itemize}
}
	
\note[item]{
    \begin{itemize}
    	\item to address this issue, there are sparse GPs\\ \lit{Snelson and Ghahramani. 2005}
    \end{itemize}
}

\end{frame}
%-----------------------------------------------------------------------

%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
%\begin{frame}[c]{Gaussian Processes - reminder}
%
%\begin{itemize}
%    \item<1-3> The prior is a GP with constant mean and variance; draws are jointly Gaussian
%    \item<2-3> The kernel (covariance) function $K$ tells us how correlated the function values at two points are
%    \item<3-3> The posterior is also a GP, with predictive distribution:
 %   \begin{equation*}
 %        P(\func_{\bocount+1} \vert \dataset_{1:\bocount}, \conf_{\bocount+1}) =  \mathcal{N}(\mean_{\bocount}(\conf_{\bocount+1}), \variance_{\bocount}(\conf_{\bocount+1}))
 %   \end{equation*}
 %   \begin{equation*}
 %       \mean_{\bocount}(\conf_{\bocount+1}) = \bm{k}^{T} \bm{K}^{-1} \bm{\func_{1:\bocount}}
 %   \end{equation*}
 %   \begin{equation*}
 %       \variance_{\bocount}(\conf_{\bocount+1}) = k(\conf_{\bocount+1}, \conf_{\bocount+1}) - \bm{k}^{T} \bm{K}^{-1} \bm{k}
 %   \end{equation*}
%\end{itemize}
%
%\note[item]{for the review of GPs - Rasmussen and Williams}
%
%\end{frame}
%-----------------------------------------------------------------------
 
% %-----------------------------------------------------------------------
% %-----------------------------------------------------------------------
% \begin{frame}[c]{Surrogate Models: GPs - kernel hyperparameters}

% \begin{itemize}
%     \item After choosing the kernel, we must also manage the hyperparameters that govern its behaviour, as well as that of the mean function.  \pause
%     \item For our problems of interest, typically we have $D + 3$ GPs hyperparameters:  
%     \begin{itemize}
%         \item $D$ length scales $\theta_{1:D}$, 
%         \item the covariance amplitude $\theta_{0}$, 
%         \item the observation noise $\noise$, 
%         \item a constant mean $\mean$. 
%     \end{itemize}
% \end{itemize}

% \end{frame}
% %-----------------------------------------------------------------------


%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}[c]{Gaussian Processes (GPs): Kernel Hyperparameters}

\begin{columns}[T] % align columns
\begin{column}{.6\textwidth}
\begin{itemize}
    \item We could optimize GP hyperparameters (maximum likelihood, MLE, or maximum a posteriori, MAP)


    \item<+-> But \alert{sampling} GP  hyperparameters from the posterior distribution performs better; e.g., via \alert{Markov-Chain Monte-Carlo (MCMC)}


    \item<+-> \alert{Marginalize} over GP hyperparameters $\theta$ and compute an \alert{integrated acquisition function}:
        \begin{equation*}
        \begin{aligned}
            \Bar{\acq}(\conf) = \int \acq (\conf, \surro_\theta)p(\theta)d\theta
        \end{aligned}
        \end{equation*}

 
    \item<+-> Downside: computational expense
    \myit{
        \item MCMC is computationally expensive
        \item Acquisition function now has to be calculated for each sample
    }
\end{itemize}
\end{column}
%
\begin{column}{.4\textwidth}
\only<2->{
    \centering
    \includegraphics[width=0.7\textwidth]{images/surrogate_models/kernel_hp_mcmc.jpg}
    
    \footnotesize{Image source: \lit{\href{https://arxiv.org/pdf/1502.05700.pdf}{Snoek et al. 2015}}}
}


   
\end{column}

\end{columns}

\end{frame}
%-----------------------------------------------------------------------
\begin{frame}[c]{Random Forests (RFs): Reminder \& How To Compute Uncertainties}

\centering
    \includegraphics[width=0.5\textwidth]{images/surrogate_models/random_forest_pic}

\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}

\begin{block}{RF Training}
\begin{itemize}
	\item Fit a set of \alert{randomized} regression trees
	\item Randomization via bootstrapping \& random selection of split variables / split points
	\item Each tree yields a possible explanation for the observations
\end{itemize}
\end{block}
\end{column}

\pause
\hfill

\begin{column}{.48\textwidth}
    \begin{block}{RF Prediction}
    \begin{itemize}
    	\item Predict with each tree
    	\item Aggregate predictions (e.g., average)
    	\item Uncertainty estimate:\\ \alert{empirical variance across tree predictions}
    \end{itemize}
    \end{block}
\end{column}
\end{columns}

\end{frame}
%-----------------------------------------------------------------------
\begin{frame}[c]{Random Forests (RFs): Impact of Basic Model Choices}
\vspace{-25pt}

\begin{columns}
\column{0.5\textwidth}

\begin{figure}[h]
\captionsetup[subfigure]{position=top}
\centering

\renewcommand{\thesubfigure}{a}
\subfloat[][no bootstrapping, \\ no random splits]{
\includegraphics[width=0.55\textwidth, clip]{images/surrogate_models/rf_noboot_middle_split.png}

}
\qquad
\renewcommand{\thesubfigure}{b}
\subfloat[][with bootstrapping, \\ no random splits]{
\includegraphics[width=0.55\textwidth, clip]{images/surrogate_models/rf_boot_middle_split.png}
}
\end{figure}

\column{0.5\textwidth}
\begin{figure}[h]
\captionsetup[subfigure]{position=top}
\centering

\pause
\renewcommand{\thesubfigure}{c}
\subfloat[][no bootstrapping, \\ with random splits]{
\includegraphics[width=0.55\textwidth, clip]{images/surrogate_models/rf_noboot_rand_split.png}

}
\qquad
\renewcommand{\thesubfigure}{d}
\subfloat[][with bootstrapping, \\ with random splits]{
\includegraphics[width=0.55\textwidth, clip]{images/surrogate_models/rf_boot_rand_split.png}
}

\end{figure}
\end{columns}

\end{frame}
%-----------------------------------------------------------------------
\begin{frame}[c]{Random Forests (RFs): Overview of Pros and Cons}

\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}

    \begin{block}{Advantages}
    \begin{itemize}
        \item Cheap to train 
        \item Scales well with \#observations $n$: 
        \begin{itemize}
        	\item Fitting: $O(n \log n)$ 
        	\item Prediction: $O(\log n)$
        \end{itemize}
        \item Scales well with \#dimensions
        \item Training can be parallelized 
        \item Can easily handle conditional, categorical, continuous and discrete spaces 
        \item Quite robust against its own hyperparameters
    \end{itemize}
    \end{block}
\end{column}%

\hfill%
\pause

\begin{column}{.48\textwidth}
    \begin{block}{Disadvantages}
    \begin{itemize}
        \item Poor uncertainty estimates 
        \item Poor extrapolation (constant) 
    	\item Priors cannot be incorporated easily 
    \end{itemize}
    \end{block}

\pause
\bigskip
\bigskip
\hspace*{0.5cm}These qualities make RFs a \alert{robust} \\
\hspace*{0.5cm}\alert{option} for Bayesian optimization in \\ \hspace*{0.5cm}\alert{high dimensions}, for \alert{categorical spaces},\\
\hspace*{0.5cm}or when function evaluations are quite fast

\end{column}
\end{columns}

\end{frame}
%-----------------------------------------------------------------------
\begin{frame}[c]{Bayesian Neural Networks: Overview}

\begin{itemize}
    \item Neural networks are more flexible \& scalable than Gaussian processes 
    \item But for use in Bayesian optimization, neural networks need to be made probabilistic
    
\pause
    \item Bayesian deep learning aims to deal with all sources of uncertainty \pause
    \begin{itemize}
        \item E.g., we don't have a single weight vector anymore, but a distribution over weights
    \end{itemize}

\centering
\includegraphics[width=0.6\textwidth]{images/surrogate_models/bnn.jpg}

\footnotesize{Image source: \lit{\href{http://proceedings.mlr.press/v37/blundell15.pdf}{Blundell et al. 2015}}}

\end{itemize}



\end{frame}
%-----------------------------------------------------------------------
%\begin{frame}[c]{Surrogate Models: Bayesian Neural Networks - Idea}
%
%\begin{itemize}
%    \item Extend regression NNs to model uncertainty
%    \item Deal with all sources of parameter uncertainty
%    \item If possible, one would also deal with uncertainty about the network architecture
%     \begin{itemize}
%        \item For every architecture, one would still want to be Bayesian about its weights... 
%        \item Nobody is really Bayesian about architectures these days 
%        \item This has been too expensive; but that may change with efficient gradient-based architecture search methods...
%    \end{itemize}
%\end{itemize}
%
%\end{frame}

%-----------------------------------------------------------------------

\begin{frame}[c]{Simplest Way of Incorporating Uncertainty in Neural Networks: DNGO}


\begin{itemize}
    \item Fit a standard regression neural network to the data (with a linear output layer)
    \item Use the representation in the last hidden layer as \alert{basis functions $\phi(x)$} of the input $x$ 
    \item \alert{Use Bayesian linear regression with these basis functions} \pause
    \begin{itemize}
        \item The last layer is linear in its parameters $\theta$  
        \item Therefore, the Bayesian linear regression formulas work directly 
        \item Feasible in closed form, in time $O(N d^3)$, where $N$ is the number of data points\\ and $d$ is the number of hidden units in the last layer 
    \end{itemize}
    \item Not fully Bayesian yet, but already allows scalable Bayesian optimization \lit{\href{https://arxiv.org/pdf/1502.05700.pdf}{Snoek et al. 2015}}

\end{itemize}

%\vspace{1cm}
%\hspace{12cm}
%\lit{\href{https://arxiv.org/pdf/1502.05700.pdf}{Snoek et al. 2015}}

\end{frame}


%-----------------------------------------------------------------------

\begin{frame}[c]{Bayesian Optimization with BNNs: Overview of Existing Approaches}

\begin{itemize}
	    \item \lit{\href{https://arxiv.org/pdf/1502.05700.pdf}{Snoek et al. 2015}} Scalable Bayesian Optimization Using Deep Neural Networks (DNGO)
	    \item \lit{\href{https://papers.nips.cc/paper/6117-bayesian-optimization-with-robust-bayesian-neural-networks.pdf}{Springenberg et al. 2016}} Bayesian Optimization with Robust Bayesian Neural Networks
	    \item \lit{\href{https://arxiv.org/abs/1706.01825}{Hern\'andez-Lobato et al. 2017}} Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space
\bigskip
\pause
        \item \lit{\href{https://www.ismll.uni-hildesheim.de/pub/pdfs/schilling2015-ecml.pdf}{Schilling et al. 2015}} Hyperparameter Optimization with Factorized Multilayer Perceptrons
        \item \lit{\href{https://papers.nips.cc/paper/7917-scalable-hyperparameter-transfer-learning}{Perrone et al. 2018}} Scalable Hyperparameter Transfer Learning

\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
\begin{frame}[c]{Bayesian Neural Networks (BNNs): Overview of Pros and Cons}

\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}

    \begin{block}{Advantages}
    \begin{itemize}
        \item Scales linearly with \#observations 
        \item Can obtain nice and smooth uncertainty estimates 
        \item Flexibility: handling of categorical, continuous and discrete spaces
    \end{itemize}
    \end{block}

\onslide<3->{
\bigskip
\bigskip
\hspace*{0.5cm}These qualities make BNNs an \\
\hspace*{0.5cm}\alert{ever-more promising alternative}
%\hspace*{0.5cm}Needed: robust auto-tuned(?) implementation
}
\end{column}%

\hfill%

\begin{column}{.48\textwidth}
\onslide<2->{
    \begin{block}{Disadvantages}
    \begin{itemize}
    	\item Usually needs more data than Gaussian processes
    	\item Uncertainty estimates often worse than for Gaussian processes 
        \item Many meta-design decisions 
    	\item No robust off-the-shelf implementation 
    \end{itemize}
    \end{block}
}
\end{column}
\end{columns}

\end{frame}
%-----------------------------------------------------------------------
\begin{frame}[c]{Bayesian Neural Networks (BNNs): Further Reading}

There is a lot more work on BNNs that hasn't been applied to Bayesian optimization yet:
\begin{itemize}
%        \item \lit{\href{https://www.cs.utoronto.ca/~radford/bnn.book.html}{Neal 1995}} Bayesian Learning for Neural Networks
%        \item \lit{\href{https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf}{Bishop 2006}} Pattern Recognition and Machine Learning
        \item Ensembles obtained simply by running SGD several times \lit{\href{https://arxiv.org/abs/1612.01474}{Lakshminarayanan et al. 2017}}
        %: Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles
        \item Dropout \lit{\href{https://arxiv.org/abs/1506.02142}{Gal et al. 2016}} 
        %Dropout as a Bayesian Approximation:
        Representing Model Uncertainty in Deep Learning
        \item Monte Carlo Batch Normalization \lit{\href{https://arxiv.org/abs/1802.06455}{Teye et al. 2018}} %Bayesian Uncertainty Estimation for Batch Normalized Deep Networks
        \item Snapshot Ensembles \lit{\href{https://arxiv.org/abs/1704.00109}{Gao Huang et al. 2017}} 
        %Snapshot Ensembles: Train 1, get M for free
\end{itemize}


\end{frame}
%-----------------------------------------------------------------------
\begin{frame}[c]{Questions to Answer for Yourself / Discuss with Friends}

\begin{itemize}
    %GP
%    \item \alert{Repetition.} What are the most important hyperparameters of a GP that you would want to optimize for Bayesian Optimization? 
    %RF
    \item \alert{Discussion.} For which optimization problems would you rather use a RF than a GP? When would you use a BNN?
\medskip
%BNN
%    \item \alert{Discussion.} Can a BNN be trained with standard MCMC in theory and in practice?
    %DNGO
    \item \alert{Discussion.} Why can DNGO's Bayesian Linear Regression approach only be applied to the last layer of a Deep Neural Network, not to all layers?
\medskip    
    \item \alert{Open Research Project.} All of the surrogate models we saw have pros and cons. It would be interesting to select the best model (and its hyperparameters) dependent on the data at hand. 

\end{itemize}
\end{frame}

%\end{document}
