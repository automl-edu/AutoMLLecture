% !TeX spellcheck = en_US

\input{../latex_main/main.tex}

\title[AutoML: L2L RL]{AutoML: Dynamic Configuration \& Learning}
\subtitle{Learning to Learn: Reinforcement Learning}
\author[Marius Lindauer]{Bernd Bischl \and Frank Hutter \and Lars Kotthoff\newline \and \underline{Marius Lindauer} \and Joaquin Vanschoren}
\institute{}
\date{}



% \AtBeginSection[] % Do nothing for \section*
% {
%   \begin{frame}{Outline}
%     \bigskip
%     \vfill
%     \tableofcontents[currentsection]
%   \end{frame}
% }

\begin{document}
	
	\maketitle
	

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning to Optimize via Reinforcement Learning \litw{\href{https://arxiv.org/abs/1606.01885}{Li and Malik'17}}}

\centering
\includegraphics[width=0.4\textwidth]{images/l2o_comic}

\tiny
Source: \url{https://bair.berkeley.edu/blog/2017/09/12/learning-to-optimize-with-rl/}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning to Optimize via Reinforcement Learning \litw{\href{https://arxiv.org/abs/1606.01885}{Li and Malik'17}}}

\begin{block}{Reinforcement Learning for Learning to Optimize}
\begin{description}
\item[State] current location, objective values and gradients evaluated at the current and past locations
\pause
\item[Action] Step update $\Delta \x$
\pause
\item[Transition] $\x^{(t)} \leftarrow \x^{(t-1)} + \Delta \x$
\pause
\item[Cost/Reward] Objective value at the current location
\begin{itemize}
\item Since the RL agent will optimize the cumulative cost, this is equivalent to $\loss_{\text{sum}}$ \lit{Chen et al'17} ($\gamma=0$)
\item encourages the policy to reach the minimum of the objective function as quickly as possible
\end{itemize}
\pause
\item[Policy] DNN predicting $\mu_d$ of Gaussian (with constant variance $\sigma^2$)\\ for dimension $d$; sample $\Delta \x_d \sim \mathcal{N}(\mu_d, \sigma^2)$
\pause
\item[Training Set] randomly generated objective functions
\end{description}
\end{block}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning to Optimize via Reinforcement Learning Results \litw{\href{https://arxiv.org/abs/1606.01885}{Li and Malik'17}}}

\centering
\includegraphics[width=0.4\textwidth]{images/l2o_dnn}

\begin{itemize}
\item 2-layer DNN with ReLUs
\item Training datasets for training RL agent:\\ four multivariate Gaussians and sampling 25 points from each
\begin{itemize}
\item[$\leadsto$] hard toy problem
\end{itemize}
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c, fragile]{Learning Acquisition Functions \litw{\href{https://arxiv.org/abs/1904.02642}{Volpp et al.'19}}}

\begin{itemize}
\item Instead of learning everything, it might be sufficient to \alert{learn hand-design heuristics}
\pause
\item In Bayesian Optimization (BO), the most critical hand-design heuristic is\\ the acquisition function
\begin{itemize}
\item trade-off between exploitation and exploration
\item Depending on the problem at hand, you might need a different acquisition function
\pause
\item Choices:
\begin{itemize}
\item probability of improvement (PI)
\item expected improvement (EI)
\item upper confidence bounds (UCB)
\item entropy search (ES) -- quite expensive!
\item knowledge gradient (KG)
\item ...
\end{itemize} 
\end{itemize}
\pause
\item \alert{Idea:} Learn a \emph{neural acquisition function} from data
\end{itemize}

$\leadsto$ Replace acquisition function 

\end{frame}
%----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}[c,fragile]{Bayesian Optimization: Algorithm}

\begin{algorithm}[H]
\Input{Search Space $\mathcal{X}$,
black box function $f$, 
\alert{acquisition function $\alpha$,}
maximal number of function evaluations $\bobudget$
}
\BlankLine
$\mathcal{D}^{(0)}$ $\leftarrow$ initial\_design($\mathcal{X}$); \\
\For{\bocount = $1, 2, \ldots \bobudget - |D_0|$}{
$\surro: \x \mapsto \cost(\x)$ $\leftarrow$ fit predictive model on $\mathcal{D}^{(\bocount-1)}$;\\
select $\x^{(\bocount)}$ by optimizing $\x^{(\bocount)} \in \argmax_{\x \in \mathcal{X}} \alert{\alpha(\x; \mathcal{D}^{(\bocount-1)}, \surro)}$;\\
Query $y^{(\bocount)} := f(\x^{(\bocount)})$;\\
Add observation to data $D^{(\bocount)} := D^{(\bocount-1)} \cup \{\langle \x^{(\bocount)}, y^{(\bocount)} \rangle \}$;
}
\Return{Best $\x$ according to $D$ or $\surro$}
\caption{Bayesian Optimization (BO)}
\end{algorithm}


\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}[c]{Neural Acquisition Function  \litw{\href{https://arxiv.org/abs/1904.02642}{Volpp et al.'19}}}

Although the \alert{acquisition function $\alpha$} depends on the history $\mathcal{D}^{(\bocount-1)}$ and the predictive model $\surro$, $\alpha$ mainly makes use of the \alert{predictive mean $\mu$ and variance $\sigma^2$}.

\pause
\bigskip

Neural acquisition function (AF):

\begin{eqnarray}
\alpha_\weights(\x) = \alpha_\weights(\mu^{(\bocount)}(\x), \sigma^{(\bocount)}(\x), \x, \bocount, \bobudget) \nonumber
\end{eqnarray}

where $\weights$ are the parameters of a neural network, and $\mu$, $\sigma$, $\x$, $\bocount$, $\bobudget$ are its inputs.

%\pause 
%\begin{itemize}
%\item Since the input is not $x$, it allows to learn scalable acquisition function
%\item No calibration of hyperparameter necessary, once the neural AF is learnt
%\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}[c]{RL to train Neural AF \litw{\href{https://arxiv.org/abs/1904.02642}{Volpp et al.'19}}}

\begin{description}
\item[Policy $\policy_\weights$:] Neural acquisition function $\alpha_\weights$
\pause
\item[Episode:] run of $\policy$ on $f\in \mathcal{F}'$
\begin{itemize}
\item $\mathcal{F}$ is a set of functions we can sample functions from
\end{itemize}
\pause
\item[State $\stateRL^{(\bocount)} $:] $\mu^{(\bocount)} $ and $\sigma^{(\bocount)} $ on a set of points $\xi^{(\bocount)} $, and $t$ and $T$
\pause
\item[Action $\actionRL^{(\bocount)} $:] Sampled point $\x^{(\bocount)}  \in \xi^{(\bocount)} $
\pause
\item[Reward $\rewardRL^{(\bocount)} $:] negative simple regret: $\rewardRL^{(\bocount)} = f(\x^*) - f(\hat{\x})$
\begin{itemize}
\item assumes that we can estimate the optimal $\x^*$ for \emph{training} functions
\end{itemize}
\pause
\item[Transition probability]: Noisy evaluation of $f$ and the predictive model update
\end{description}

\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}[c]{Policy \litw{\href{https://arxiv.org/abs/1904.02642}{Volpp et al.'19}}}

\begin{itemize}
\item The state is described by a discrete set of points $\xi^{(\bocount)}  = \{\xi_n\}^N_{n=1}$
\pause
\item We feed these points through the predictive model and the neural AF to obtain $\alpha_\weights(\xi_n) = \alpha_\weights(\mu^{(\bocount)} (\xi_n), \sigma^{(\bocount)}(\xi_n), \xi_n, t, T),  $
\pause
\item $\alpha_\weights(\xi_i)$ are interpreted as the logits of categorical distribution, s.t.
$$\pi_\alpha(\cdot \mid s^{(\bocount)} ) = \text{Cat}\left[\alpha_\weights(\xi_1), \ldots, \alpha_\weights(\xi_N) \right] $$
\pause
\item Due to curse of dimensionality, we need a two step approach for~$\xi^{(\bocount)} $
\begin{enumerate}
\item sample $\xi_{\text{global}}$ using a coarse Sobol grid
\item sample $\xi_{\text{local}}$ using local optimization starting from the best samples in $\xi_{\text{global}}$
\end{enumerate}
\item[$\leadsto$] $\xi^{(\bocount)} = \xi_{\text{global}} \cup \xi_{\text{local}}$ 
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}[c,fragile]{Learning Acquisition Functions: Overview \litw{\href{https://arxiv.org/abs/1904.02642}{Volpp et al.'19}}}

\centering
\includegraphics[width=0.8\textwidth]{images/l2acq.png}


\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}[c]{Results on Artificial Functions \litw{\href{https://arxiv.org/abs/1904.02642}{Volpp et al.'19}}}

\centering
\includegraphics[width=0.9\textwidth]{images/l2acq_results.png}

\medskip

\begin{itemize}
\item Approach by \lit{Volpp et al. '19} called MetaBO
\item MetaBO performs better than other acquisition functions\\ (EI, GP-UBC, PI) and other baselines (Random, TAF)
\end{itemize}

\pause
\begin{flushleft}
\alert{Assumption}: You have a family of functions at hand that resembles your target function.	
\end{flushleft}


\end{frame}
%-----------------------------------------------------------------------


\end{document}
