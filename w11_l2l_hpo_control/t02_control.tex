
\input{../latex_main/main.tex}



\title[AutoML: DAC]{AutoML: Dynamic Configuration \& Learning}
\subtitle{Dynamic Configuration}
\author[Marius Lindauer]{Bernd Bischl \and Frank Hutter \and Lars Kotthoff\newline \and \underline{Marius Lindauer} \and Joaquin Vanschoren}
\institute{}
\date{}



% \AtBeginSection[] % Do nothing for \section*
% {
%   \begin{frame}{Outline}
%     \bigskip
%     \vfill
%     \tableofcontents[currentsection]
%   \end{frame}
% }

\begin{document}
	
	\maketitle
	

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Iterative Optimization Heuristics}
	
	\begin{itemize}
		\item Many iterative heuristics in algorithms are dynamic and adaptive
		\begin{enumerate}
			\item the algorithm's behavior changes over time
			\item the algorithm's behavior changes based on internal statistics
		\end{enumerate}
		\medskip
		\pause
		\item These heuristics might control other hyperparameters of the algorithms
		\pause
		\smallskip
		\item Example: learning rate schedules for training DNNs
		\begin{enumerate}
			\item exponential decaying learning rate: based on number of iterations, learning rate decreases
			\pause
			\item Reduce learning rate on plateaus: if the learning stagnates for some time,\\ the learning rate is decreased by a factor
		\end{enumerate}
		\pause
		\smallskip
		\item other examples: restart probability of search, mutation rate of evolutionary algorithms, \ldots  
		
	\end{itemize}
	
\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Parametrization of Learning Rate Schedules}
	
	\begin{itemize}
		\item How can we parameterize learning rate schedules?
		\begin{enumerate}
			\item exponential decaying learning rate:
			\begin{itemize}
				\item initial learning rate
				\item minimal learning rate
				\item multiplicative factor
			\end{itemize}
			\pause
			\item Reduce learning rate on plateaus:
			\begin{itemize}
				\item patience (in number of epochs)
				\item patience threshold
				\item decreasing factor
				\item cool-down break (in number of epochs)
			\end{itemize}
		\end{enumerate}
		\pause
		\medskip
		\item[$\leadsto$] Many hyperparameters only to control a single hyperparameter
		\pause   
		\item Still not guaranteed that optimal setting of e.g. learning rate schedules will lead to optimal learning behavior
		\begin{itemize}
			\item Learning rate schedules are only heuristics
		\end{itemize}
	\end{itemize}
	
\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Dynamic Algorithm Configuration}

\begin{itemize}
	\item So far, we assumed that an algorithm runs with static settings
	\item However, settings, such as learning rate, have to be adapted over time
\end{itemize}

\begin{block}{Definition}
	Let 
	\begin{itemize}
		\item $\conf \in \pcs$ be a hyperparameter configuration of an algorithm $\algo$,
		\pause
		\item $p(\dataset)$ be a probability distribution over meta datasets $\dataset \in \datasets$,
		\pause
		\item $\stateRL_t$ be a state description of $\algo$ solving $\dataset$ at time point $t$,
		\pause
		\item $\cost: \policies \times \datasets \to \perf$ be a cost metric assessing the cost of a control policy $\pi \in \Pi$ on $\dataset \in \datasets$
	\end{itemize}
	
	\pause
	the \emph{dynamic algorithm configuration problem} is to obtain a policy $\policy^* : \stateRL_t \times \dataset \mapsto \conf$ by optimizing its cost across a distribution of datasets:
	\begin{equation}
	\policy^* \in \argmin_{\policy \in \policies} \int_{\datasets} p(\dataset) \cost(\policy, \dataset) \diff \dataset \nonumber
	\end{equation}
\end{block}

\end{frame}
%-----------------------------------------------------------------------	

%----------------------------------------------------------------------
\begin{frame}[c]{Dynamic Algorithm Configuration as Contextual MDP \litw{\href{https://ml.informatik.uni-freiburg.de/papers/20-ECAI-DAC.pdf}{Biedenkapp et al. 2020}}}


\begin{description}
	\item[State $\stateRL_t$] are described by statistics gathered in the algorithm run
	\pause
	\item[Action $\actionRL_t$] change hyperparameters according to some control policy $\pi$
	\pause
	\item[Transition] run the algorithm from state $s_t$ to $s_{t+1}$ for a "short" moment by using the hyperparameters defined by $a_t$
	\pause
	\item[Reward $\rewardRL_t$] Return your current solution quality (or an approximation)
	\pause
	\item[Context $\dataset$] A given dataset (or task)
\end{description}

\bigskip
	
\centering
\input{tikz/control.tex}
	
	
\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Solving Dynamic Algorithm Configuration}

Solve unknown MDP by using reinforcement learning (RL):

\begin{equation}
\mathcal{V}_\dataset^\policy(s_t) =  \mathbb{E} \left[\sum_{k=0}^\infty\gamma^k \rewardRL^{(t+k+1)}_\dataset| \stateRL_t=\stateRL \right]\nonumber
\end{equation}
\pause

\begin{equation}
 = \mathbb{E} \left[ \rewardRL^{(t+1)}_\dataset+\gamma\mathcal{V}_\dataset^\policy(\stateRL^{(t+1)})| \stateRL^{(t+1)} \sim \mathcal{T}_\dataset(\stateRL^{(t)}, \policy(\stateRL^{(t)})) \right] \nonumber
\end{equation}
\pause
\begin{equation}
\policy^* \in
\argmax_{\policy \in \policies}
\int_{\datasets} p(\dataset) \int_{\statesRL^{(0)}} \Pr(\stateRL^{(0)}) \cdot \mathcal{V}^\policy_\dataset(\stateRL^{(0)}) \diff \stateRL^{(0)} \diff \dataset \nonumber
\end{equation}

\bigskip
\pause
$\leadsto$ equivalent to Dynamic Algorithm Configuration definition


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Dynamic Algorithm Configuration across Datasets \litw{\href{https://ml.informatik.uni-freiburg.de/papers/20-ECAI-DAC.pdf}{Biedenkapp et al. 2020}}}
	
\begin{itemize}
	\item Challenge: Evaluating a policies on all datasets is often not feasible
	\pause
	\item Curriculum learning \lit{Bengio et al. 2009} showed that we should have a curriculum of tasks we tackle
	\pause
	\item Self-paced learning \lit{Kumar et al. 2010} tries to automatically find such as a curriculum
	\begin{itemize}
		\item Focus on "easy" tasks where the agent can improve most:
	\end{itemize}
\end{itemize}
	
\pause
\begin{equation} 
\label{spl_loss}
\max_{\policy,\vec{v}}\mathcal{C}(\policy, \vec{v}, K) = \sum^{|\datasets|}_{i=1} \vec{v}_i\mathcal{R}_i(\policy) - \frac{1}{K} \sum^{|\datasets|}_{i=1} \vec{v}_i \nonumber
\end{equation}

with $\weights$ being the agent's policy parameters and\newline $\vec{v}$ being a masking vector for choosing the tasks at hand.

%\pause
%\medskip
%
%Iterative greedy optimization of $\vec{v}$:
%\begin{equation}
%\vec{v}_i = \left\{
%\begin{array}{ll}
%1, &  \mathrm{if}\quad\mathcal{C}(\policy, \vec{v}_{i}:=0, K) \leq \mathcal{C}(\policy, \vec{v}_{i}:=1, K)\\
%0, & \mathrm{otherwise}
%\end{array}
%\right.\nonumber
%\end{equation}
\end{frame}
%----------------------------------------------------------------------

\end{document}
