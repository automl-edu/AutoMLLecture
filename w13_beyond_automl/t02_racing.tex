% !TeX spellcheck = en_US

\input{../latex_main/main.tex}

\title[AutoML: Racing]{AutoML: Beyond AutoML}
\subtitle{Racing for Algorithm Configuration}
\author[Marius Lindauer]{Bernd Bischl \and Frank Hutter \and Lars Kotthoff\newline \and \underline{Marius Lindauer} \and Joaquin Vanschoren}
\institute{}
\date{}
\week{13}
\topicnumber{2}


% \AtBeginSection[] % Do nothing for \section*
% {
%   \begin{frame}{Outline}
%     \bigskip
%     \vfill
%     \tableofcontents[currentsection]
%   \end{frame}
% }

\begin{document}
	
	\maketitle
	
%-----------------------------------------------------------------------
\begin{frame}[c]{State-of-the-art Algorithm Configuration}

SMAC: Sequential Model-based Algorithm Configuration \lit{\href{https://ml.informatik.uni-freiburg.de/papers/11-LION5-SMAC.pdf}{Hutter et al. 2011}}

\begin{itemize}
	\item Bayesian Optimization +
	\item aggressive racing +
	\item adaptive capping (for optimizing runtime)
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}[c]{SMAC: Overview \litw{\href{https://ml.informatik.uni-freiburg.de/papers/11-LION5-SMAC.pdf}{Hutter et al. 2011}}}
%\\\litw{Hutter et al. 2011}}

\LinesNotNumbered
\begin{algorithm}[H]
\Input{%
	instance set $\insts$,
	Algorithm $\algo$ with configuration space $\confs$,
	Initial configuration $\conf_0$,
	performance metric $c$,
	Configuration budget $b$
}
%\Output{best incumbent configuration $\finconf$}
\BlankLine
run history $\hist$ $\leftarrow$ initial design based on $\conf_0$; \tcp*{$\hist = (\conf, \inst, c(\inst,\conf))_i$}
\While{$b$ remains} {
	\pause
	$\epm \leftarrow$ train empirical performance model based on run history $\hist$;\\
	\pause
	$\confs_{challengers} \leftarrow$ select configurations based on $\epm$;\\
	\pause
	$\finconf, \hist$  $\leftarrow$ intensify($\confs_{challengers}$, $\finconf$); \tcp*{racing and capping}
}
\pause
\Return{$\finconf$}
\caption{SMAC}
\end{algorithm}

\end{frame}
%-----------------------------------------------------------------

%-----------------------------------------------------------------------
\begin{frame}[c]{Comparisons on $N$ instances \litw{\href{https://ml.informatik.uni-freiburg.de/papers/09-JAIR-ParamILS.pdf}{Hutter et al. 2009}}}

\begin{itemize}
\item \alert{Basic(N)} uses a pretty basic comparison: \textit{better$_N(\conf', \conf)$}:
\begin{itemize}
\item Compare $\conf'$ and $\conf$ based on $N$ instances 
\pause
\item How does this relate to cross-validation? 
\end{itemize}  

\bigskip
\pause
\item Problem: How to set $N$? Problems of large $N$? Small $N$? 
\pause
\begin{itemize}
\item Problem of large $N$: evaluations are slow
\item Problem of small $N$: overfitting to a small set of instances
\item[$\leadsto$] Tradeoff: Choose $N$ of moderate size 
\end{itemize}

\end{itemize}
\end{frame}
%-----------------------------------------------------------------------


%-----------------------------------------------------------------------
\begin{frame}[c]{Comparisons on $N$ instances \litw{\href{https://ml.informatik.uni-freiburg.de/papers/09-JAIR-ParamILS.pdf}{Hutter et al. 2009}}}


Question: \alert{Which} $N$ instances should we use? 
\begin{enumerate}
\item $N$ different instances for each configuration
\item The same set of $N$ instances for the entire run
\end{enumerate}

\bigskip
\pause
Answer: the same $N$ instances, so that we compare apples with apples
\begin{itemize}
\item[] (but: using the same instances can also yield overtuning) 
\end{itemize}
\bigskip

\pause
If we sampled different instances for each configuration:
\begin{itemize}
\item Some configurations would randomly get easier instances
\item Those configurations would look better than they really are
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------



%-----------------------------------------------------------------------
\begin{frame}[c]{Comparisons on $N$ instances \litw{\href{https://ml.informatik.uni-freiburg.de/papers/09-JAIR-ParamILS.pdf}{Hutter et al. 2009}}}

Question: For randomized algorithms, how should we set the seeds? 
\begin{enumerate}
\item Sample a new seed for each algorithm run
\item Fix the seeds together with the instances
\end{enumerate}
\bigskip
\pause
Answer: just like for instances, fix them to compare apples to apples

\bigskip
\pause
In summary, for each run of Basic(N): \\pick $N$ (instance, seed) pairs and use them for evaluating each $\conf$.\\
\pause
(Different Basic(N) runs can use different instances and seeds.)

\end{frame}
%-----------------------------------------------------------------------


%-----------------------------------------------------------------------
\begin{frame}[c]{The concept of overtuning}

Very related to overfitting in machine learning 
\begin{itemize}
\item Performance improves on the training set
\item Performance does not improve on the test set, and may even degrade
\end{itemize}	

\pause
\medskip

More pronounced for heterogeneous benchmark sets 
\begin{itemize}
\item But it even happens for very homogeneous sets
\item Indeed, one can even overfit on a single instance, to the \alert{seeds} used for training 
\end{itemize}	

\end{frame}
%-----------------------------------------------------------------------

%-----------------------------------------------------------------------
\begin{frame}[fragile]{Overtuning Visualized}

\begin{itemize}
	\item Example: minimizing SLS solver runlengths for a single SAT instance
	\item \alert{Training cost}, e.g., with N=100:\\average runlengths across 100 runs with different seeds
	\item \alert{Test cost} of $\finconf$ here based on 1000 new seeds 
\end{itemize}	

\pause


\begin{center}
	\only<2>{\includegraphics[scale=0.13]{images/basicils100_training.png}}
	\only<3>{\includegraphics[scale=0.13]{images/basicils100_training_and_test.png}}
\end{center}


\end{frame}
%-----------------------------------------------------------------------

%-----------------------------------------------------------------------
\begin{frame}[fragile]{Basic(N) Test Results with Various N}

\begin{itemize}
\item Example: minimizing SLS solver runlengths for a single SAT instance
\item \alert{Training cost}, e.g., with N=?:\\average runlengths across N runs with different seeds
\item \alert{Test cost} of $\finconf$ here based on 1000 new seeds 
\end{itemize}	

\pause

\begin{multicols}{2}
\begin{center}
	\includegraphics[scale=0.2]{images/basicils_unlabled.png}
\end{center}
\columnbreak{}
\pause
Which of these results corresponds to $N=1$, $N=10$, and $N=100$?\\


\pause
\medskip

\begin{enumerate}
	\item N=1: blue, N=10: red,\\ N=100 dashed black
	\item N=1: dashed black,\\ N=10: red, N=100 blue
\end{enumerate}

\pause
Correct Answer: 1


\end{multicols}


\end{frame}
%-----------------------------------------------------------------------



%-----------------------------------------------------------------------
\begin{frame}[c,fragile]{Aggressive Racing (inspired by FocusedILS) \litw{\href{https://ml.informatik.uni-freiburg.de/papers/09-JAIR-ParamILS.pdf}{Hutter et al. 2009}}}

{Intuition: get the best of both worlds}

\begin{itemize}
	\item Perform more runs for good configurations
	\begin{itemize}
		\item[-] to avoid overtuning
	\end{itemize}
	\item Quickly reject poor configurations
	\begin{itemize}
		\item[-] to make progress more quickly
	\end{itemize}
\end{itemize}

\pause
\medskip

\begin{block}{Definition: $N(\conf)$ and $c_N(\conf)$}
\alert{$N(\conf)$} denotes the number of runs executed for $\conf$ so far.\\
\alert{$\hat{c}_N(\conf)$} denotes the cost estimate of $\conf$ based on $N$ runs.
\end{block}

\pause
In the beginning: $N(\conf)=0$ for every configuration $\conf$

\end{frame}

%-----------------------------------------------------------------------



%-----------------------------------------------------------------------
\begin{frame}[c,fragile]{Aggressive Racing (inspired by FocusedILS) \litw{\href{https://ml.informatik.uni-freiburg.de/papers/09-JAIR-ParamILS.pdf}{Hutter et al. 2009}}}


\begin{block}{Definition: domination}
\alert{$\confI1$ dominates $\confI2$} if 
\begin{itemize}
\item $N(\confI1) \ge N(\confI2)$ and 
\item $\hat{c}_{N(\confI2)}(\confI1) \le \hat{c}_{N(\confI2)}(\confI2)$.
\end{itemize}
I.e.: we have at least as many runs for $\confI1$ and its cost is at least as low.
\end{block}

\pause

\begin{block}{\textit{better($\conf', \finconf$)} in a nutshell}
\begin{itemize}
\item $\finconf$ is the current configuration to beat (incumbent)
\pause
\item Perform runs of $\conf'$ until either
\begin{itemize}
\item $\finconf$ dominates $\conf'$ $\leadsto$ reject $\conf'$, or
\item $\conf'$ dominates $\finconf$ $\leadsto$ change current incumbent $(\finconf \leftarrow \conf')$
\end{itemize}
\pause	
\item Over time: perform extra runs of $\finconf$ to gain more confidence in it
\end{itemize}
\end{block}

\end{frame}

%-----------------------------------------------------------------------


%-----------------------------------------------------------------------
\begin{frame}[c,fragile]{Toy Example}


\begin{itemize}
\item Let $\finconf$ be the incumbent  (evaluated on $\instI1, \instI2, \instI3$)
\item We'll look at challengers $\conf'$ and $\conf''$
\end{itemize}

\begin{center}
\begin{tabular}{l ccc}
& $\instI1$ & $\instI2$ & $\instI3$ \\
\hline
$\finconf$ 	& 3 		& 2			& 10	\onslide<2->\\
\hline
$\conf'$		& \onslide<3->{2}			& \onslide<4->{10} 		& \\
& 			& \onslide<5->{$\to$ reject, since $\hat{c}_2(\conf')=6 > \hat{c}_2(\finconf)=2.5$} & \\
\hline
\onslide<6->{$\conf''$}		& \onslide<6->{3}			& \onslide<7->{1} 		& \onslide<8->{5}\\
\end{tabular}
\end{center}

\onslide<9->
\begin{itemize}
\item new incumbent: $\finconf \leftarrow \conf''$
\item Perform an additional run for new $\finconf$ to increase confidence over time
\end{itemize}


\end{frame}
%-----------------------------------------------------------------------


%-----------------------------------------------------------------------
\begin{frame}[fragile]{Racing achieves the best of both worlds}

{Aggressive racing (aka FocusedILS): Fast progress and no overtuning}

\begin{center}
Test performance\\
\only<1>{\includegraphics[scale=0.25]{images/basicils.png}}
\only<2>{\includegraphics[scale=0.25]{images/focusedils.png}}
\end{center}

\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}[fragile]{Overview of Racing}
\LinesNotNumbered
\begin{algorithm}[H]
	\scriptsize
	\Input{%
		 candidate configurations $\confs_{new}$, cutoff $\cutoff_{max}$, previously evaluated runs $\hist$, budget $\bobudget$, incumbent $\finconf$}
		\While{$\confs_{new}$ not empty}{
			$\confI{\bocount} \xleftarrow{} $ getNext$(\confs_{new})$;\\
			\pause
			$\inst, s \xleftarrow{}$ instance and seed drawn uniformly at random;\\
			$c$ $\xleftarrow{}$ EvaluateRun$(\finconf, \inst, s, \cutoff_{max})$;\\
			$\hist \xleftarrow{} \hist \cup (\finconf, \inst, s, \cost)$;\\
			\pause
			\While{true}{
				 $\insts^+, \mathbf{s}^+ \xleftarrow{} $getAlreadyEvaluatedOn$(\finconf, \hist)$;\\
				 $\insts^{(\bocount)}, \mathbf{s}^{(\bocount)} \xleftarrow{} $getAlreadyEvaluatedOn$(\confI{\bocount}, \hist)$;\\
				$\instI{\bocount}, s^{(\bocount)} \xleftarrow{}$ drawn uniformly at random from $\insts^+ \setminus \insts^{(\bocount)}$ and $\mathbf{s}^+ \setminus \mathbf{s^{(\bocount)}}$;\\
				\pause
%				$\mathbf{s_i} \xleftarrow{} \mathbf{s_i} \cup s_i$;\\
%				$\cutoff_i \xleftarrow{}$ AdaptCutoff$(\cutoff_{max}, \langle(\conf_j, \cost_j)\rangle_{\conf_j = \conf^+})$;\\
				$c_i \xleftarrow{}$ EvaluateRun$(\confI{\bocount}, \instI{\bocount},  s^{(\bocount)}, \cutoff_{max})$;\\
				$\hist \xleftarrow{} \hist \cup (\confI{\bocount}, \instI{\bocount},  s^{(\bocount)},  \cost^{(\bocount)})$;\\
				\pause
				\If{average cost of $\confI{\bocount} >$ average cost of $\finconf$ across $\insts^{(\bocount)}$ and $\mathbf{s}^{(\bocount)}$}{break;}
				\pause
				\ElseIf{average cost of $\confI{\bocount} <$ average cost of $\finconf$ and $\insts^+ =  \insts^{(\bocount)}$  and $\mathbf{s}^{(\bocount)} = \mathbf{s}^+$}{
					$\finconf \xleftarrow{} \confI{\bocount}$;
					}
			}
		\pause
		\If{time spent exceeds $\bobudget$ or $\confs_{new}$ is empty} {
			\Return{$\finconf, \hist$}
		}
		}
\end{algorithm}
\end{frame}
%-----------------------------------------------------------------------


\end{document}
