% !TeX spellcheck = en_US

\input{../latex_main/main.tex}

\title[AutoML: Capping]{AutoML: Beyond AutoML}
\subtitle{Best Practices for Algorithm Configuration}
\author[Marius Lindauer]{Bernd Bischl \and Frank Hutter \and Lars Kotthoff\newline \and \underline{Marius Lindauer} \and Joaquin Vanschoren}
\institute{}
\date{}
\week{13}
\topicnumber{5}



% \AtBeginSection[] % Do nothing for \section*
% {
%   \begin{frame}{Outline}
%     \bigskip
%     \vfill
%     \tableofcontents[currentsection]
%   \end{frame}
% }

\begin{document}
	
	\maketitle
	

%----------------------------------------------------------------------
\begin{frame}[c]{Pitfalls \& Best Practices}

The \alert{goals} of automated algorithm configuration include:

\begin{enumerate}
	\pause
	\item reducing the expertise required to use an algorithm
	\pause
	\item less human-time
	\pause
	\item tuning algorithms to the task at hand
	\pause
	\item faster development of algorithms
	\pause
	\item facilitating systematic and reproducible research
\end{enumerate}

\bigskip
\alert{BUT}:

\begin{enumerate}
	\item algorithm configuration can lead to over-tuning
	\pause
	\item using algorithm configuration requires (at least some) expertise in algorithm configuration
	\pause
	\item if done wrong, waste of time and compute resources
\end{enumerate}

\end{frame}
%-----------------------------------------------------------------------

%----------------------------------------------------------------------
\begin{frame}[c]{Setting up AC}

$9$ Steps to your well-performing algorithm:

\begin{enumerate}
	\item Define your performance metric 
	\pause
	\item Define instance set
	\pause
	\item Split your instances in training and test
	\pause
	\item Define the configuration space
	\pause
	\item Choose your preferred configurator
	\pause
	\item Implement an interface between your algorithm and the configurator
	\pause
	\item Define resource limitations of your algorithm
	\pause
	\item Run the configurator on your algorithm and the training instances
	\pause
	\item Validate the eventually returned configuration on your test instances
\end{enumerate}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Pitfall 1: Trust your algorithm}

We have encountered algorithms that
\begin{itemize}
	\item ignored resource limitations
	\item returned wrong solutions
	\item even returned negative runtimes
\end{itemize}

\bigskip
\pause

\begin{block}{Best Practice 1: Never trust your algorithm}
	Explicitly check and use external software to:
	\begin{enumerate}
		\item ensure resource limitations
		\item terminate your algorithm
		\item verify returned solutions
		\item measure performance 
	\end{enumerate}
\end{block}

\end{frame}
%-----------------------------------------------------------------------

%----------------------------------------------------------------------
\begin{frame}[c]{Pitfall 2: File System}

Algorithm configurators ...
\begin{itemize}
	\item often produce quite some log files (e.g., for each algorithm run)
	\item  are often used on HPC clusters with a shared file system
\end{itemize}

\pause
$\leadsto$ It's surprisingly easy to substantially slow down a shared file system

\pause
\bigskip

\begin{block}{Best Practice 2: Don't use the Shared File System}

To relieve the file system of a HPC cluster:
\begin{itemize}
	\item design well which files are required and which are not
	\item use a local (SSD) disc
\end{itemize}
	
\end{block}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Pitfall 3: Over-tuning}

It's easy to overtune to different aspects, incl.:

\begin{itemize}
	\item training instances
	\item random seeds
	\item machine type
\end{itemize}

\bigskip
\pause

In practice, it can be hard to prevent over-tuning, e.g., by

\begin{itemize}
	\item using larger instance sets
	\item tuning on the target hardware
\end{itemize}

\begin{block}{Best Practice 3: Check for Over-Tuning}
Check for over-tuning by validating your final configuration on
\begin{itemize}
	\item many random seeds
	\item a large set of unused test instances
	\item a different hardware
\end{itemize}
\end{block}
	
\end{frame}
%-----------------------------------------------------------------------

%----------------------------------------------------------------------
\begin{frame}[c]{Pitfall 4: Heterogeneous Instance Sets}

Algorithm configurators...
\begin{itemize}
	\item use some kind of racing to not evaluate each configuration on all instances
	\pause
	\item can be mislead on subsets of instances if the instance set is heterogeneous
\end{itemize}

\pause
$\leadsto$ returned configurations often perform worse than default configurations\\ in the validation phase

\bigskip
\pause

\begin{block}{Best Practice 4: Ensure Homogeneity}
	Algorithm configurators should only run on homogeneous instance sets.
	
	Different degrees of homogeneity:
	\begin{itemize}
		\item Strong homogeneity: all instances agree on the ranking of configurations
		\item Weak homogeneity: all instances agree on the top-performing configurations
	\end{itemize}

\end{block}

\end{frame}
%-----------------------------------------------------------------------

%----------------------------------------------------------------------
\begin{frame}[c]{More Pitfalls and Best Practices}

... can be found in 
\lit{\href{https://arxiv.org/pdf/1705.06058.pdf}{Eggensperger et al. 2019}}


\end{frame}
%-----------------------------------------------------------------------


\end{document}
