\input{t00_template.tex}
\subtitle{Example and Practical Hints}



\begin{document}

\maketitle


%----------------------------------------------------------------------
%----------------------------------------------------------------------

\begin{frame}[allowframebreaks]{Tuning Ranges}

\begin{itemize}
	\item Knowledge about hyperparameters can help to guide the optimization
	\item For example, it can be beneficial to sample hyperparameters on a non-uniform scale.
\end{itemize}

    \vspace{0.2cm}
Example: regularization hyperparameter ($C$ or \emph{cost}) of SVM: $[2^{-15}, 2^{15}] \approx [0.000031, 32768]$

\begin{itemize}
	\item The distance between $100$ and $200$ should be the same as between $0.1$ and $0.2$.
  \item We might want to sample here from from a log-scale, e.g., $[2^{\conf_l}, 2^{\conf_u}]$ with $\conf_l = -15$ and $\conf_u = 15$.
\end{itemize}

\begin{figure}[htb]
\centering

  \begin{tikzpicture}[auto]%[scale=1.5]
    \draw [->](-0.3,0)-- (12.3,0) coordinate;
    \draw [->](-0.3,-2)-- (12.3,-2) coordinate;
    % \foreach \x/\xtext/\xxtext in {-15/-15/0.000031, -10/-10/ , -5/-5/, 0/0/1, 5/5/32, 10/10/1024, 15/15/32768} {
    \def\xM{15} % max exponent
    \def\xW{12} % max width in cm
    \foreach \x/\xtext/\xxtext in {-15/-15/0, -10/-10/ , -5/-5/, 0/0/, 5/5/, 10/10/1024, 15/15/32768} {
      \def\xA{{(\x + \xM) * (\xW / (2 * \xM))}} %untrafoed val in cm (0 to 12)
      \def\xB{{\xW * 2^(\x-\xM)}} %trafoed val in cm
      \draw [very thick] (\xA,-2pt) -- ++(0, 4pt) node[xshift = -6pt, yshift=-3pt,anchor=south west,baseline]{\strut$\xtext$};
      \draw [very thick] (\xB,-2cm+2pt) -- ++(0,-4pt) node[anchor=north]{\scriptsize $\xxtext$};
      \draw [->] (\xA,-2pt) .. controls (\xA,-0.5) and (\xB,-1.5) .. (\xB,-2cm+2pt);
    }
    \node[] at (-0.7,-0.1) (t1) {$\conf$};
    \node[] at (-0.7,-1.9) (t2) {$2^{\conf}$};
  \end{tikzpicture}

\end{figure}

\framebreak

    \begin{itemize}
        \item Similar to the scale, e.g., linear or logarithmic, the ranges in which to search have to be specified.
        \item Deep knowledge about the inner working of the machine learning algorithm can help to choose better bounds.
        \item If $\finconf$ is close to the border of $\pcs$ the ranges should be increased (or a different scale should be selected).
        \item Meta-Learning can help to decide which hyperparameters should be tuned in which ranges.
    \end{itemize}
    % \vspace{0.5cm}
    % Example:
    % \begin{itemize}
    %         \item The ranges for $\text{cost} \in [10^{-3}, 10^3]$ and $\gamma \in [10^{-3}, 10^3]$ are rather small.
    %         \item More common would be ranges like $\text{cost} \in [2^{-15}, 2^{15}]$ and $\gamma \in [2^{-15}, 2^{15}]$.
    % \end{itemize}

\end{frame}




\begin{frame}{Tuning Example - Setup}

    We want to train a \textit{spam detector} on the popular Spam dataset\footnote{\url{https://archive.ics.uci.edu/ml/datasets/spambase}}.

    \begin{itemize}
        \item The learning algorithm is a support vector machine (SVM) with RBF kernel.
        \item The hyperparameters we optimize are
            \begin{itemize}
                \item Cost parameter $\text{cost} \in [2^{-15}, 2^{15}]$.
                \item Kernel parameter $\gamma \in [2^{-15}, 2^{15}]$.
            \end{itemize}
        \item We compare four different optimizer
            \begin{itemize}
                \item Random search
                \item Grid search
                \item A $(\mu+\lambda)$-selection evolutionary algorithm with $\mu = 1$, $\lambda = 1$ and Gauss mutation with $\sigma = 1$.
                \item \textit{CMAES} - an evolutionary algorithm that generates offspring from a multivariate normal distribution (We didn't cover this algorithm).
            \end{itemize}
        \item We use a 5-fold cross-validation and optimize the accuracy (ACC).
        \item All methods are allowed a budget of $25$ evaluations.
    \end{itemize}

\end{frame}

\begin{frame}{Tuning Example}

\begin{columns}
\begin{column}{0.45\textwidth}
  \vspace{1em}
  % \resizebox{\linewidth}{!}{
  %   \begin{tabular}{l|l|l|l}
  %   Parameter&Type & Min & Max \\
  %   \hline
  %   \texttt{cost}  & double & $10^{-3}$ & $10^{3}$ \\
  %   \texttt{gamma} & double & $10^{-3}$ & $10^{3}$ \\
  %   \end{tabular}
  % }

  We notice here:

  \begin{itemize}
      \item Both \emph{Grid search} and \emph{random search} have many evaluations in regions with bad performance ($\gamma>1$).
      \item \emph{CMAES} only explores a small region.
      \item \emph{(1+1)-EA} does not converge.
  \end{itemize}
\end{column}%
\begin{column}{0.5\textwidth}
  \vspace{-1em}
  \begin{figure}
  \includegraphics[width=0.9\textwidth]{images/benchmark_scatter.png}
  \end{figure}
\end{column}
\end{columns}

\end{frame}
\begin{frame}{Tuning Example cont.}

The \emph{optimization curve} shows the best found configuration until a given time point.
\begin{columns}
\begin{column}{0.4\textwidth}
  \footnotesize
  \only<1>{

    Note:

    \begin{itemize}
      \item For \emph{random search} and \emph{grid search} the chronological order on the x-axis is arbitrary.
      \item The curve shows the performance on the tuning validation (\emph{inner resampling}) on a single fold
    \end{itemize}
  }
  \only<2-3>{
    \begin{itemize}
      \item<2-> The outer 10-fold CV gives us 10 optimization curves.
      \item<3-> The median at each time point gives us an estimate of the average optimization progress.
    \end{itemize}
  }
  \only<4>{
    \begin{itemize}
      \item Remember: The final model will be trained on the \emph{outer training set} with the configuration $\finconf$ that lead to the best performance on the \emph{inner test set}.
      \item To compare the effectiveness of the tuning strategies we have to look at the performance that $\finconf$ gives us on the \emph{outer test set}.
    \end{itemize}
  }
\end{column}
\begin{column}{0.6\textwidth}
  \vspace{-1em}
  \begin{figure}
  \only<1>{\includegraphics[width=\textwidth]{images/benchmark_curve_iter_1.png}}
  \only<2>{\includegraphics[width=\textwidth]{images/benchmark_curve_iter_all.png}}
  \only<3>{\includegraphics[width=\textwidth]{images/benchmark_curve_median.png}}
  \only<4>{\includegraphics[width=\textwidth]{images/benchmark_curve_iter_all_median.png}}
  \end{figure}
\end{column}
\end{columns}


  %   \footnotesize
  %     \item Tuning does not necessarily improve the performance of the learner, because e.g.\ tuning is badly configured or default values are determined by \emph{clever} heuristics.
  %     \item Tuning error can be overly optimistic (see \emph{nested resampling}).
  %   \end{itemize}
  % }
  % \only<2>{
  %   \begin{itemize}
  %     \item Effect of the chosen resampling split (objective noise) can dominate tuning effect.
  %   \end{itemize}
  % }
\end{frame}

\begin{frame}{Tuning Example: Validation}

\begin{columns}
\begin{column}{0.4\textwidth}
  \footnotesize

  The box plots show the distribution of the ACC-values that were measured on the \emph{outer test set} with a 10-fold CV.

  Note:

  \begin{itemize}
    \item The box plots do not indicate significant differences.
    \item The performance differs from the results obtained on the inner resampling.
  \end{itemize}

\end{column}
\begin{column}{0.6\textwidth}
  \vspace{-1em}
  \begin{figure}
  \includegraphics[width=\textwidth]{images/benchmark_boxplot_tuners.png}
  \end{figure}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Tuning Example: Validation}

\begin{columns}
\begin{column}{0.4\textwidth}
  \footnotesize

  \begin{itemize}
    \item Comparison with an SVM that was configured with $\conf = (\text{cost},\gamma) = (1,1)$ shows that tuning is necessary.
  \end{itemize}
\end{column}
\begin{column}{0.6\textwidth}
  \begin{figure}
  \includegraphics[width=\textwidth]{images/benchmark_boxplot_default.png}
  \end{figure}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Data Dependent Defaults}

    \begin{itemize}
            \item Static defaults of hyperparameters, e.g., $\conf = (\text{cost},\gamma) = (1,1)$ are rarely a good choice.
            \item A simple extension is to compute defaults based on some simple dataset characteristics.
            \item The best know example is the formula for the size of the random subset of features to consider as a split in a random forest: $\sqrt{p}$, where $p$ is the number of features.
            \item For the RBF-SVM a data dependent default for the $\gamma$ parameter can be computed by
                \begin{itemize}
                     \item The pairwise distances $\|\x - \tilde \x \|$ between points of a random subset containing $50\%$ of the data points are calculated.
                     \item The estimate is based upon the $0.1$ and $0.9$ quantile of these distances.
                     \item Basically any value between those two bounds will produce good results.
                     \item Take the mean of the $0.1$ and $0.9$ quantile of these distances as an estimate for $\gamma$.
                \end{itemize}
            \item These simple defaults can work well and don't require expensive tuning procedures.
    \end{itemize}
\end{frame}


\begin{frame}{Tuning Example: Validation}

\begin{columns}
\begin{column}{0.4\textwidth}
  \footnotesize

  \begin{itemize}
      \item With the previously discussed data depended default of $\gamma$ and $\text{cost} = 1$.
  \end{itemize}
\end{column}
\begin{column}{0.6\textwidth}
  \begin{figure}
  \includegraphics[width=\textwidth]{images/benchmark_boxplot_all.png}
  \end{figure}
\end{column}
\end{columns}
\end{frame}
%\begin{frame}{Practical Hints: Nested Resampling}
%\begin{itemize}
%   \item For small data sets the relative size of the \emph{inner training set} should not differ much from \emph{outer training set}. Example: \\$n = 200$,
%   \begin{itemize}
%     \item outer resampling: 5-fold CV, inner resampling: 3-fold CV: $n_{\text{inner train}} = \frac{4}{5} \cdot {2}{3} \cdot n = 0.53 * n = 107$
%     \item inner and outer resampling: 10-fold CV: $\frac{9}{10} \cdot \frac{9}{10} \cdot n = 0.81 * n = 162$
%   \end{itemize}
%   \item Resampling strategies depend on dataset sizes:
%  \begin{itemize}
%    \item Small datasets: More resampling iterations necessary to obtain reliable performance estimate (e.g.\ repeated CV).
%    \item Large datasets: Less resampling iterations possible due to runtime, but holdout can be sufficient to estimate performance.
%    \item For unbalanced and multi-class datasets $n$ has to be higher to obtain reliable performance estimates, i.e.\ they should be treated like small datasets if not sufficiently big.
%  \end{itemize}
%\end{itemize}
%\end{frame}


\end{document}
